What to do:
- Define what kind of tasks to do: demo the methods
- Define terms
- Find a problem to solve. For now, in general, the optimization problem is all about finding global maximum or minimum of an objective function f

Each method should have:
- Demo
- Advantage, disadvantage
- When to use

NOISY DESCENT
Add a Gaussian noise at each descent step
    x^(k+1) <- x^(k) + alpha * g^(k) + epsilon^(k)
where epsilon^(k) ~ N(mu, sigma)

MESH ADAPTIVE DIRECT SEARCH
Use random positive spanning directions

SIMULATED ANNEALING
At every iteration, a candidate transition from x to x' (it's like a gradient descent step that updates x by x') is sampled from a distribution T (?). Let delta_y = f(x') - f(x). The transition is accepted with probability:
    accept_probability = 1 if delta_y <= 0 (it's decreasing, horray)
                         e^(-delta_y/t) if delta_y > 0
t is the temperature, which starts out high and then is decreased over time

Some way to update t
* Logarithmic annealing schedule: t^(k) = t^(1) * ln(2) / ln(k+1)
* Exponential annealing schedule: t^(k+1) = gamma * t^(k)
* Fast annealing: t^(k) = t^(1) / k

CROSS ENTROPY
Maintain a proposal distribution. At each iteration, we sample from the proposal distribution and then update the proposal distribution itself to fit a collection of the best samples (what?).

Requirement: choosing a family of proposal distribution parameterized by theta. Example: multivariate normal distribution. At each iteration, we fit the mean and covariance matrix using a number of elite samples (how do I know which is elite?)
